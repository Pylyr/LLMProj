\documentclass[12pt,a4paper]{article}

% Quelques options d'encodage, de langues et de format du document
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[top=2cm, bottom=3cm, left=1.75cm, right=1.75cm]{geometry}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{graphicx} % Pour la commande "\includegraphics"
\usepackage{hyperref} % Pour la commande "\url"
\pagenumbering{arabic}



\begin{document}

\begin{center}
  \begin{tabular}{|p{0.2\textwidth}|p{0.75\textwidth}|}
    \hline
    {
    \vspace{0cm} % without it, bugs, don't know why?
    \centerline{\includegraphics[width=\linewidth]{tp-ipp.png}}
    }
    & {
      \vspace{0cm} % same here
      \centering
      \large
      {\hfill January, 2025}
      
      \vspace*{.5cm}
      \textbf{APM\_5AI29\_TP}
      
      \vspace*{.5cm}
      \setstretch{1.5}
      {\Large\textbf{Language Models and Structured Data}}
      
      \vspace*{.5cm}
      Mid-term Project Report

      \vspace*{1cm}
      %{\hfill\href{http://teaching.simplicitytheory.science}{teaching.simplicitytheory.science}}
      } \\
    \hline
  \end{tabular}
\end{center}

\noindent Acronym of the Team: Table Turners\\
Name:	Dmitry Timkin, Ivanina Ivanova, Mark Daychman, Renata Mindiyarova

{\centering\rule{\linewidth}{.5pt}}


%\maketitle
\begin{center}
\section*{Enhancing Text-to-Table Information Extraction with Updated BART Models}
\end{center}
\section*{Abstract}
In this paper, we revisit the proposed 'text-to-table' information extraction task, which converts unstructured text content into structured tables using sequence-to-sequence models. We explain the core idea of the original paper \cite{wu2022texttotablenewwayinformation} to use table constraints and relation embeddings to improve accuracy. We test the approach on new datasets and develop an improved preprocessing and an ML-based error correction for an improved accuracy. 

\section*{Problem Statement}
Information extraction (IE) is one of the fundamental tasks in natural language processing. It aims to convert text into more dense and structured knowledge representations, making the information more accessible for further applications. One of the examples of such structured data type are tables. Before this paper was released, IE into table usually required predefined schemas, making it inflexible and domain specific. Accurate text-to-table generation, especially without a predefined schema, has many real-life applications across various domains, including automated structured reports from legal documents, financial statements, and medical records. 
\ \\

The paper formalizes this task as a sequence-to-sequence (seq2seq) problem and proposes a solution based on BART~\cite{lewis2019bart}, which is known for its strong performance in text generation. The proposed method incorporates additional techniques such as table constraints and table relation embeddings to ensure that the generated tables are correctly structured. 
\ \\

Table constraints are implemented during the encoding process to enforce that the tables have a consistent structure. Specifically, the model determines the number of columns from the first generated header row and ensures that every generated row has the same number of columns. This prevents incomplete rows or irregular table formats. This method improves the syntactic accuracy of the model's output.
\ \\

Table relation embeddings help with the table cell alignment by incorporating their relationship with their respective row and column headers. During the table generation, the model uses row and column relation embeddings to identify which row each non-header cell belongs to. These embeddings are added as relational vectors in the self-attention function. This helps improve the table's coherence and accuracy. 
\ \\

On top of that the original authors of the paper correctly identify the greatest weakness of their proposed approach -- the model performs worse with larger input texts. To help alleviate this weakness, we introduced 2 new approaches. We modified the preprocessing to also include a small summarization LLM to condense the text, so that the BART core model has a smaller input. We also added an extra validation step using a graph neural network (GNN) to ensure that the generated tables are correct. 
\ \\

Finally, we were also curious to see how the text-to-table approach generalizes with new data. We wanted to not only recreate the original results, but also use the preprocessing tools available in the repository to generate new data to test the model. Since, the reverse problem, i.e. 'table-to-text' has been fairly well-studied; one can use any existing table database, generate the textual summaries using one of the table-to-text models and use it as the input for the 'text-to-table' model. The accuracy can then be measured by comparing the original and reproduced tables. 
\ \\

Overall, in this paper we aim to address the following key questions:
\begin{itemize}
    \item Does the original approach generalize well? Does it work equally well with technical jargon within areas like medicine and law? 
    \item Does introducing an additional summarization preprocessing step boost the model's accuracy?
    \item Can we further improve the accuracy of the model by adding a validator? 
\end{itemize}




\section*{Method/Overall Architecture}

\begin{itemize}
\item A screenshot of you running the preprocessing tool to generate new data. Ez extra page. [RENATA]
\item What do you use to summarize? Is there a token restriction? What about the cases when the table is suppose to be large? [RENATA]
\item What is our gnn? Why did we use it? How did we use it? Some details [RENATA]
\end{itemize}

\section*{Experimentation}
\textit{Note: We faced several challenges running the original code from the paper due to its reliance on outdated Python versions and deprecated modules. The installation required extensive manual version tuning, use of an older pip version, and the manual cloning of certain packages from GitHub repositories to resolve compatibility issues. }
\ \\
\subsection*{Used Datasets}
The original paper uses the following 4 datasets that we have all also included in our testing:

\begin{itemize}

\item{\textbf{Rotowire}} is a sports domain dataset containing basketball game reports. Each instance consists of a long text report and two tables representing team and player scores. This dataset is challenging due to the long-form text that includes irrelevant information, making information extraction difficult.

\item{\textbf{E2E}} is a restaurant domain dataset where each instance is a short text description of a restaurant paired with an automatically constructed table summarizing its characteristics. It has a limited set of table texts, resulting in low diversity, which makes generalization difficult.

\item{\textbf{WikiTableText}} is an open-domain dataset where each instance consists of a short text description and a table with row headers collected from Wikipedia. It captures structured information with a balance between textual descriptions and tabular data.

\item{\textbf{WikiBio}} is extracted from Wikipedia biography pages. Each instance contains a biography introduction and a table from the infobox of the corresponding Wikipedia page. The text is significantly longer than the table and contains more information, making it useful for evaluating models that process rich textual content.
\end{itemize}
On top of that, we also introduce two new datasets:
\begin{itemize}
\item{\textbf{Dataset 1}} [RENATA]
\item{\textbf{Dataset 2}} [RENATA]
\end{itemize}

\subsection*{Accuracy Metric}

To assess the accuracy of the generated tables, the paper employs precision, recall, and the F1 score. These metrics are applied to both headers and non-header cells to measure their correctness. Precision (\( P \)) is defined as the fraction of correctly predicted results among all predicted results:

\[
P = \frac{1}{|y|} \sum_{y \in y} \max_{y^* \in y^*} O(y, y^*)
\]

where \( O(\cdot) \) denotes the similarity between a predicted and ground-truth value. Recall (\( R \)) measures the fraction of correct predictions relative to the total ground-truth entries:

\[
R = \frac{1}{|y^*|} \sum_{y^* \in y^*} \max_{y \in y} O(y, y^*)
\]

The F1 score is the harmonic mean of precision and recall:

\[
F1 = \frac{2}{\frac{1}{P} + \frac{1}{R}}
\]

For similarity computation \( O(\cdot) \), three approaches are considered:
\begin{itemize}
    \item \textbf{Exact Match:} Checks if the predicted text exactly matches the ground-truth text.
    \item \textbf{chrf Score:} Computes character-level n-gram similarity between predicted and ground-truth values.
    \item \textbf{BERTScore:} Measures similarity using contextual embeddings from BERT.
\end{itemize}

Non-header cells are evaluated using both their content and associated headers. This ensures that the cell belongs to the correct row and column. If the header text is slightly different but semantically equivalent, similarity is computed using the chrf or BERTScore measures rather than an exact match. Empty cells are ignored during evaluation as they do not contain useful information. The summary of the results can be found in Figure 


\begin{table}[h]
    \centering
    \begin{tabular}{llccc|ccc}
        \toprule
        \textbf{Dataset} & \textbf{Model} & \multicolumn{3}{c}{\textbf{Row Header F1}} & \multicolumn{3}{c}{\textbf{Non-header Cell F1}} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8}
         & & Exact & ChrF & BERT & Exact & ChrF & BERT \\
        \midrule
        \multirow{3}{*}{E2E} 
        & Original & 91.23 & 92.40 & 95.34 & 90.80 & 90.97 & 92.20 \\
        & Summarization & 99.62 & 99.69 & 99.88 & 97.87 & 97.99 & 98.56 \\
        & Summ. + Valid. & \textbf{99.63} & \textbf{99.69} & \textbf{99.88} & \textbf{97.88} & \textbf{98.00} & \textbf{98.57} \\
        \midrule
        \multirow{3}{*}{WikiTableText} 
        & Original & 59.72 & 70.98 & 94.36 & 52.23 & 59.62 & 73.40 \\
        & Summarization & 78.15 & 84.00 & 95.60 & \textbf{59.26} & \textbf{69.12} & 80.69 \\
        & Summ. + Valid. & \textbf{78.16} & \textbf{83.96} & \textbf{95.68} & 59.14 & 68.95 & \textbf{80.74} \\
        \midrule
        \multirow{3}{*}{WikiBio} 
        & Original & 63.99 & 71.19 & 81.03 & 56.51 & 62.52 & 61.95 \\
        & Summarization & \textbf{80.53} & \textbf{84.98} & \textbf{92.61} & 68.98 & 77.16 & 76.54 \\
        & Summ. + Valid. & 80.52 & 84.96 & 92.60 & \textbf{69.02} & \textbf{77.16} & \textbf{76.56} \\
        \midrule
        \multirow{3}{*}{Rotowire} 
        & Original & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX \\
        & Summarization & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX \\
        & Summ. + Valid. & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} \\
        \midrule
        \multirow{3}{*}{New Dataset 1} 
        & Original & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX \\
        & Summarization & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX \\
        & Summ. + Valid. & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} \\
        \midrule
        \multirow{3}{*}{New Dataset 2} 
        & Original & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX \\
        & Summarization & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX & XX.XX \\
        & Summ. + Valid. & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} & \textbf{XX.XX} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results on four existing and two newly introduced datasets.}
    \label{tab:results}
\end{table}

\section*{Discussion}

Comparison with expectations, limitations, lessons learned, and perspectives.

\section*{Further work}
The original study evaluates the performance of BART-base and BART-large~\cite{lewis2019bart} from 2020, which were state of the art at the time.  We think the results will significantly improve, because newer and larger models (e.g. T5~\cite{raffel2020exploring}, Flan-T5~\cite{chung2024scaling}, ModernBERT~\cite{warner2024smarter}) are specifically optimized for long-text generation. Because of all the extra tokens that are needed to properly encode a table using text, the output is usually fairly long, so long-text generation models are expected to perform better.
\ \\

We are planning to try more modern versions of the Bart model, but since the article code is implemented in the library fairseq, we encountered problems with implementing the new architecture into the code.



\bibliographystyle{plain}  % Use any style you prefer (e.g., plain, alpha, IEEE)
\bibliography{sample}



\end{document}
